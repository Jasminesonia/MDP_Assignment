{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04c67dab",
   "metadata": {},
   "source": [
    "## Part 1: Least to Most Prompting\n",
    "\n",
    "### Basic Prompt\n",
    "**Prompt:** \"Explain what a Markov Decision Process is in Pong.\"  \n",
    "**Output (GPT):**  \n",
    "A Markov Decision Process (MDP) is a framework used in reinforcement learning to model decision-making where outcomes are partly random and partly under the control of an agent.\n",
    "\n",
    "**Reflection:** This is correct but very general. It misses the role of states and rewards.  \n",
    "\n",
    "### Intermediate Prompt\n",
    "**Prompt:** \"In the context of Pong, explain how rewards work when the paddle hits or misses the ball. Include specific reward values.\"  \n",
    "**Output (GPT):**  \n",
    "In reinforcement learning, the reward function tells the agent (the paddle) whether it is doing well or poorly. In Pong, rewards are typically assigned based on whether the agent successfully hits the ball or misses it.\n",
    "\n",
    "1. Hitting the Ball\n",
    "\n",
    "When the paddle successfully hits the ball back toward the opponent, it is considered a positive action.\n",
    "\n",
    "Reward value: 0 or sometimes a small positive reward like +0.1 for encouraging good paddle behavior before actually scoring.\n",
    "\n",
    "Why 0 sometimes? Because the main reward is usually given for scoring a point, not just hitting the ball. However, giving a small positive reward can speed up learning.\n",
    "\n",
    "2. Scoring a Point\n",
    "\n",
    "When the ball passes the opponent’s paddle, the agent scores a point.\n",
    "\n",
    "Reward value: +1\n",
    "\n",
    "This is the primary positive reinforcement.\n",
    "\n",
    "3. Missing the Ball\n",
    "\n",
    "When the paddle fails to hit the ball and it passes the agent, the opponent scores.\n",
    "\n",
    "Reward value: -1\n",
    "\n",
    "This is a penalty that discourages the agent from missing the ball.\n",
    "\n",
    "4. No Event\n",
    "\n",
    "While the ball is moving and nothing significant happens (no score, no miss), the reward is usually:\n",
    "\n",
    "Reward value: 0\n",
    "\n",
    "**Reflection:** More useful since it connects to the actual game.  \n",
    "\n",
    "| Event                  | Reward    |\n",
    "| ---------------------- | --------- |\n",
    "| Paddle hits ball       | 0 or +0.1 |\n",
    "| Paddle misses ball     | -1        |\n",
    "| Paddle scores a point  | +1        |\n",
    "| Ball in play, no score | 0         |\n",
    "\n",
    "**Summary**\n",
    "\n",
    "Positive reward (+1): scoring a point\n",
    "\n",
    "Negative reward (-1): missing the ball\n",
    "\n",
    "Optional small reward (+0.1): hitting the ball\n",
    "\n",
    "Neutral reward (0): ball in play without scoring\n",
    "\n",
    "This reward structure teaches the agent to maximize its points while minimizing misses, which is the goal of Pong.\n",
    "\n",
    "\n",
    "### Advanced Prompt\n",
    "**Prompt:** \"Act as a reinforcement learning expert. Explain how MDPs, rewards, and returns work together in Pong, using paddle position and ball trajectory as state examples.\"  \n",
    "**Output (GPT):**  \n",
    "In reinforcement learning, the MDP defines the interaction between the agent (paddle) and environment (ball).\n",
    "\n",
    "Example State (s): Paddle at y=5, Ball at (10, 5) moving left-down.\n",
    "\n",
    "Example Action (a): Move paddle down by 1 unit.\n",
    "\n",
    "The Transition updates the state: ball moves left and down, paddle moves to y=6.\n",
    "\n",
    "The Reward (R): If the paddle aligns with the ball and bounces it, reward = +1. Otherwise, –1.\n",
    "\n",
    "The Return (G): The total discounted sum of rewards over future steps, e.g.,\n",
    "\n",
    "\n",
    "\t\tG0​=R1​+γR2​+γ2R3​+…\n",
    "\n",
    "\n",
    "Thus, the paddle learns to maximize returns by repeatedly moving to states that give higher chances of bouncing the ball back.\n",
    "\n",
    "\n",
    "\n",
    "**Reflection:** This ties concepts together and starts showing how the theory maps to the game.  \n",
    "\n",
    "\n",
    "### Expert Prompt\n",
    "**Prompt:** \"First explain MDPs, then show how rewards accumulate into returns in Pong. Verify your explanation by describing how the paddle would learn optimal positioning.\"  \n",
    "**Output (GPT):**  \n",
    "An MDP is defined by states (paddle and ball positions), actions (paddle movements), transitions (ball trajectory and bounce), and rewards (+1 for hit, –1 for miss).\n",
    "\n",
    "In Pong:\n",
    "\n",
    "Suppose at time step 0, the paddle moves to intercept the ball. Reward = +1.\n",
    "\n",
    "At time step 1, the paddle continues tracking the ball. Reward = +1.\n",
    "\n",
    "The Return (G) is calculated using a discount factor γ, such as:\n",
    "\n",
    "\t\tG0​=1+0.9(1)+0.92(1)+…\n",
    "\n",
    "This produces a decreasing sequence of returns (e.g., 4.1, 3.44, 2.71, …).\n",
    "\tThe paddle learns optimal positioning by trial and error. Over many games, it sees that moving towards the ball maximizes the total return, while ignoring the ball leads to negative returns. Eventually, it develops the policy: “Track the ball’s y-position to maximize returns.\n",
    "**Reflection:** This is the deepest level. It connects MDP → rewards → return → learning strategy.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5016cb",
   "metadata": {},
   "source": [
    "## Part 2: Define the MRP Tuple in Pong\n",
    "\n",
    "- **S (States):** \n",
    "  - Ball position (x, y)\n",
    "  - Ball velocity (up, down, left, right)\n",
    "  - Paddle position (y-axis)\n",
    "\n",
    "- **P (Transition Probabilities):**\n",
    "  - Given the current state, where will the ball go next?\n",
    "  - Example: If the ball is moving downwards, 80% chance it continues down, 20% chance it bounces.\n",
    "\n",
    "- **R (Rewards):**\n",
    "  - +1 if the paddle hits the ball\n",
    "  - –1 if the paddle misses the ball\n",
    "  - 0 otherwise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a466f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: State=(0.6, np.float64(0.5)), Action=0.00, Reward=1, Return=4.10\n",
      "Step 1: State=(0.7, np.float64(0.6)), Action=0.10, Reward=1, Return=3.44\n",
      "Step 2: State=(0.7999999999999999, np.float64(0.7)), Action=0.10, Reward=1, Return=2.71\n",
      "Step 3: State=(0.8999999999999999, np.float64(0.7999999999999999)), Action=0.10, Reward=1, Return=1.90\n",
      "Step 4: State=(0.9999999999999999, np.float64(0.8999999999999999)), Action=0.10, Reward=1, Return=1.00\n"
     ]
    }
   ],
   "source": [
    "#Sample Run: State, Action, Reward, and Return\n",
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "n_steps = 5\n",
    "gamma = 0.9  # discount factor\n",
    "\n",
    "# State variables\n",
    "ball_y = 0.5       # initial ball position\n",
    "paddle_y = 0.5     # paddle starts in middle\n",
    "ball_dir = 1       # 1 = moving down, -1 = moving up\n",
    "\n",
    "states, actions, rewards, returns = [], [], [], []\n",
    "\n",
    "for t in range(n_steps):\n",
    "    # Action: move paddle towards ball (simple policy)\n",
    "    action = np.sign(ball_y - paddle_y) * 0.1\n",
    "    paddle_y += action\n",
    "\n",
    "    # Ball moves\n",
    "    ball_y += ball_dir * 0.1\n",
    "    if ball_y >= 1 or ball_y <= 0:\n",
    "        ball_dir *= -1   # bounce\n",
    "\n",
    "    # Reward\n",
    "    if abs(ball_y - paddle_y) < 0.1:\n",
    "        reward = 1\n",
    "    else:\n",
    "        reward = -1\n",
    "\n",
    "    states.append((ball_y, paddle_y))\n",
    "    actions.append(action)\n",
    "    rewards.append(reward)\n",
    "\n",
    "# Compute returns G_t\n",
    "for t in range(n_steps):\n",
    "    G = 0\n",
    "    discount = 1\n",
    "    for k in range(t, n_steps):\n",
    "        G += discount * rewards[k]\n",
    "        discount *= gamma\n",
    "    returns.append(G)\n",
    "\n",
    "# Display results\n",
    "for t in range(n_steps):\n",
    "    print(f\"Step {t}: State={states[t]}, Action={actions[t]:.2f}, \"\n",
    "          f\"Reward={rewards[t]}, Return={returns[t]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f29d39",
   "metadata": {},
   "source": [
    "\n",
    "### Interpretation of Results\n",
    "- **State:** The state is represented by the ball’s y-position and the paddle’s y-position. \n",
    "- **Action:** The action is how much the paddle moves (positive = up). In this run, the paddle consistently moved towards the ball.\n",
    "- **Reward:** Each step gave a reward of +1, meaning the paddle was aligned well with the ball.\n",
    "- **Return:** Returns decreased over time (4.10 → 1.00) because the discount factor (γ=0.9) reduces the weight of future rewards.\n",
    "- **Connection to Theory:** This matches the Bellman equation for returns:\n",
    "  G_t = R_t + γR_{t+1} + γ²R_{t+2} + ...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99a6b12",
   "metadata": {},
   "source": [
    "## 6. Reflection & Talking Points\n",
    "\n",
    "- This exercise demonstrated how an **MDP (Markov Decision Process)** can be converted into an **MRP (Markov Reward Process)** by fixing the agent’s actions.  \n",
    "- In the Pong example, the **state** was represented by the paddle and ball positions, while the **action** was the paddle movement.  \n",
    "- The simulation confirmed the **reward mechanism**: every correct paddle alignment gave a reward of +1, and the discounted return decreased step by step (4.10 → 3.44 → 2.71 → 1.90 → 1.00).  \n",
    "- This sequence matches the theoretical formula for returns:  \n",
    "  \\[\n",
    "  G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots\n",
    "  \\]  \n",
    "- The experiment showed that if the paddle consistently tracks the ball, rewards stay positive and the return value remains high. If the paddle misses, rewards would turn negative, reducing the return drastically.  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
