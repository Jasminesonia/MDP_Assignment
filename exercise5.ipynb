{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b8b1393",
   "metadata": {},
   "source": [
    "### Step 1. Recall the MDP and MRP Context ###\n",
    "\n",
    "### MDP (Markov Decision Process):###\n",
    "Defined as a tuple:\n",
    "\n",
    "    (S,A,P,R,γ)\n",
    "where:\n",
    "S: Set of states\n",
    "A: Set of actions\n",
    "P: Transition probability function\n",
    "R: Reward function\n",
    "γ: Discount factor\n",
    "\n",
    "### MRP (Markov Reward Process):###\n",
    "Once a policy π is fixed, the MDP reduces to an MRP:\n",
    "    (S,P^π,R^π,γ)\n",
    "\n",
    "### State-Value Function:###\n",
    "The value of a state under policy π:\n",
    "\n",
    "    V^π(s)=E_π​[G_t​∣S_t​=s]\n",
    "where the return is:\n",
    "    G_t = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514a7ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2. Environment Setup\n",
    "from dataclasses import dataclass\n",
    "import random\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class State:\n",
    "    paddle_y: int\n",
    "    ball_x: int\n",
    "    ball_y: int\n",
    "    vx: int\n",
    "    vy: int\n",
    "\n",
    "def step(state, action):\n",
    "    \"\"\"Transition function for simplified Pong.\"\"\"\n",
    "    paddle_y, ball_x, ball_y, vx, vy = state.paddle_y, state.ball_x, state.ball_y, state.vx, state.vy\n",
    "    \n",
    "    # Update paddle\n",
    "    if action == \"UP\":\n",
    "        paddle_y -= 1\n",
    "    elif action == \"DOWN\":\n",
    "        paddle_y += 1\n",
    "    \n",
    "    # Update ball\n",
    "    ball_x += vx\n",
    "    ball_y += vy\n",
    "    \n",
    "    # Bounce vertically\n",
    "    if ball_y <= 0 or ball_y >= 4:\n",
    "        vy *= -1\n",
    "    \n",
    "    # Reward\n",
    "    reward = 0\n",
    "    if ball_x == 0:  # ball reached left edge\n",
    "        if abs(paddle_y - ball_y) <= 1:  # paddle hits\n",
    "            reward = +1\n",
    "            vx *= -1\n",
    "        else:\n",
    "            reward = -1\n",
    "    \n",
    "    return State(paddle_y, ball_x, ball_y, vx, vy), reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65d80a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3. Compute Discounted Return\n",
    "def compute_return(rewards, gamma=0.99):\n",
    "    G = 0\n",
    "    for t, r in enumerate(rewards):\n",
    "        G += (gamma ** t) * r\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4179c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 4. Generate an Episode\n",
    "def random_policy(state):\n",
    "    return random.choice([\"UP\", \"DOWN\", \"STAY\"])\n",
    "\n",
    "def generate_episode(start_state, policy_fn, gamma=0.99, max_steps=20):\n",
    "    \"\"\"Simulate one episode.\"\"\"\n",
    "    states, actions, rewards = [], [], []\n",
    "    s = start_state\n",
    "    \n",
    "    for _ in range(max_steps):\n",
    "        a = policy_fn(s)\n",
    "        ns, r = step(s, a)\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "        s = ns\n",
    "    \n",
    "    # Compute returns (backward)\n",
    "    returns = []\n",
    "    G = 0\n",
    "    for r in reversed(rewards):\n",
    "        G = r + gamma * G\n",
    "        returns.insert(0, G)\n",
    "    \n",
    "    return states, actions, rewards, returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51df6e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 5. Estimate the State-Value Function\n",
    "from collections import defaultdict\n",
    "\n",
    "def estimate_v(num_episodes=500, gamma=0.99):\n",
    "    V = defaultdict(float)\n",
    "    counts = defaultdict(int)\n",
    "    \n",
    "    for _ in range(num_episodes):\n",
    "        s0 = State(2, 4, 2, -1, 1)\n",
    "        states, actions, rewards, returns = generate_episode(s0, random_policy, gamma)\n",
    "        \n",
    "        for s, G in zip(states, returns):\n",
    "            V[s] += G\n",
    "            counts[s] += 1\n",
    "    \n",
    "    # Average returns\n",
    "    for s in V:\n",
    "        V[s] /= counts[s]\n",
    "    \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e699a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Trajectory:\n",
      "State=State(paddle_y=2, ball_x=4, ball_y=2, vx=-1, vy=1), Action=DOWN, Reward=0, Return=-0.97, V(s)≈0.26\n",
      "State=State(paddle_y=3, ball_x=3, ball_y=3, vx=-1, vy=1), Action=DOWN, Reward=0, Return=-0.98, V(s)≈0.09\n",
      "State=State(paddle_y=4, ball_x=2, ball_y=4, vx=-1, vy=-1), Action=UP, Reward=0, Return=-0.99, V(s)≈-0.21\n",
      "State=State(paddle_y=3, ball_x=1, ball_y=3, vx=-1, vy=-1), Action=DOWN, Reward=-1, Return=-1.00, V(s)≈0.29\n",
      "State=State(paddle_y=4, ball_x=0, ball_y=2, vx=-1, vy=-1), Action=UP, Reward=0, Return=0.00, V(s)≈0.00\n"
     ]
    }
   ],
   "source": [
    "#Step 6. Sample Run\n",
    "# Estimate V(s) from many episodes\n",
    "V = estimate_v(num_episodes=1000, gamma=0.99)\n",
    "\n",
    "# Simulate one trajectory\n",
    "s0 = State(2, 4, 2, -1, 1)\n",
    "states, actions, rewards, returns = generate_episode(s0, random_policy, gamma=0.99, max_steps=5)\n",
    "\n",
    "print(\"Sample Trajectory:\")\n",
    "for s, a, r, G in zip(states, actions, rewards, returns):\n",
    "    print(f\"State={s}, Action={a}, Reward={r}, Return={G:.2f}, V(s)≈{V[s]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a638f911",
   "metadata": {},
   "source": [
    "### Step 7. Talking Points ###\n",
    "\n",
    "### State-Value Function V(s):###\n",
    "    Represents the expected return from a given state under a policy.\n",
    "    In Pong, states with the paddle near the ball generally have higher V(s).\n",
    "    States far from the ball tend to have lower or negative V(s).\n",
    "\n",
    "### Monte Carlo Estimation:###\n",
    "    Averaging returns over many episodes approximates V(s).\n",
    "    Requires multiple simulations since transitions are stochastic under a random policy.\n",
    "\n",
    "### Sample Trajectory:###\n",
    "    Shows how State, Action, Reward, Return, and Value link together.\n",
    "    The discounted return explains why early rewards matter more than later ones."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
